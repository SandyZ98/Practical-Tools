---
title: "SURVMETH 745 HW3"
author: "Stacey Frank&Chendi Zhao"
date: "2/3/2022"
output: html_document
---

```{r setup, include=FALSE}
library(PracTools)
library(tidyverse)
library(paramtest)
```
#VDK Exercise 4.1.
Write out the null and alternative hypothesis being tested.
Calculate the sample size as instructed.
```{r}
income_02=6100
income_10=7000
delta=income_10-income_02
unitRV=2.5
sd=sqrt(unitRV*income_02^2)
power.t.test(sd=sd,
             delta=delta,
             power=.8,
             type = "one.sample",
             alt = "one.sided",
             sig.level = .05)
```

##A

$H_0$:$\mu=\$7000$ 
$H_A$:$\mu<\$7000$ 
at $\alpha$=0.05 level

##B

A simple random sample size of 712 will allow us to detect that the average income has risen to $7,000.

#VDK Exercise 4.2. 

Example 4.6 is in VDK P116

##a)
```{r}
sd_1=sqrt(200/2)
power.t.test(sd=sd_1,
             delta=5,
             power=.8,
             type = "two.sample",
             alt = "one.sided",
             sig.level = .05)

power.t.test(sd=sd_1,
             delta=5,
             power=.9,
             type = "two.sample",
             alt = "one.sided",
             sig.level = .05)
```
If $\sigma^2_d=200$, the sample size with 80% power would be 51 and the sample size with 90% power would be 70.

##b)
```{r}
sd_2=sqrt(800/2)
power.t.test(sd=sd_2,
             delta=5,
             power=.8,
             type = "two.sample",
             alt = "one.sided",
             sig.level = .05)

power.t.test(sd=sd_2,
             delta=5,
             power=.9,
             type = "two.sample",
             alt = "one.sided",
             sig.level = .05)
```
If $\sigma^2_d=800$, the sample size with 80% power would be 199 and the sample size with 90% power would be 275.

##c)
When having the same power, sample size will be larger as $\sigma^2_d$ increases.
When having the same $\sigma^2_d$, sample size will be larger as power goes up.

## Simulation
Use the simulation approach introduced in the class with 10,000 iterations and check its results against results from an appropriate power function in R.
```{r}
t_fun_1<-function(simN,sampsize,mean,delta){
x1<-rnorm(sampsize,mean,sd_1)
x2<-rnorm(sampsize,mean+delta,sd_1)
t<-t.test(x1,x2,var.equal=T)
tstat<-t$statistic
p<-t$p.value
sig<-p<0.05
return(c(t=tstat,p=p,sig=sig))
}
t_power_1<-grid_search(t_fun_1,
params=list(sampsize=c(51,70),
            mean=seq(50,90,10),
delta=5),
n.iter=10000,output='data.frame')

results(t_power_1)%>%
group_by(sampsize.test, delta.test)%>%
summarize(power=mean(sig))


```

```{r}
t_fun_2<-function(simN,sampsize,mean,delta){
x1<-rnorm(sampsize,mean,sd_2)
x2<-rnorm(sampsize,mean+delta,sd_2)
t<-t.test(x1,x2,var.equal=T)
tstat<-t$statistic
p<-t$p.value
sig<-p<0.05
return(c(t=tstat,p=p,sig=sig))
}
t_power_2<-grid_search(t_fun_2,
params=list(sampsize=c(199,275),
            mean=seq(50,90,10),
delta=5),
n.iter=10000,output='data.frame')


results(t_power_2)%>%
group_by(sampsize.test, delta.test)%>%
summarize(power=mean(sig))

power.t.test(n = c(51,70,199,275), delta = 5, sd =c(sd_1,sd_1,sd_2,sd_2),
sig.level = 0.05,
type = c("two.sample"),
alternative = c("one.sided"))
```
We observed that the mean can affect the value of power, therefore we set the mean weight from 50kg-100kg in the simulation function. The results indicate that, for sample size equals to 51,70,199,275, the powers are 0.70,0.83,0.70,0.83, which are slightly smaller than the corresponding result obtained from an appropriate power function.  Therefore, without knowing the value of mean weight, we are not always able to detect a difference in mean weight of 5kg between males and females with the sample size calculated above.

#VDK Exercise 4.11.

##a
```{r}

nDep2sam(S2x=(55^2),S2y=(55^2), g=.6, r=1, rho=.76, alt="two.sided", del= 7.2, sig.level=.05, pow=.80)

```
A sample size of 499 employees will be needed in January to detect a change of 10% (either an increase or decrease) between January and July with a power of .80. 

##b
```{r}

nDep2sam(S2x=(55^2),S2y=(55^2), g=1, r=1, rho=.76, alt="two.sided", del= 7.2, sig.level=.05, pow=.80)

#January sample size with 100% overlap is 220; adjust for 60% retest rate
adj_samp <- 220/.6
adj_samp

```
If you assume that only overlapping cases between January and July will be used, you need at least 220 people to participate at both time points to detect a 10% change in scores. Given that we only expect 60% of people to participate at both time points, we divide 220 by .60 to get an attrition-adjusted January sample size of 367 people. 

##c
```{r}
#formulas on pg 119

##Part A variance of estimated mean difference

cov_xy <- .76*(.55*.55)
  
var_diff_a <- (1/499)*((55^2)+(1*(55^2))-(2*.6*1*(cov_xy)))


##Part B variance of estimated mean difference

var_diff_b <- ((2*(55^2))*(1-.76))/220

var_ratio <- var_diff_a/var_diff_b

n_ratio <- 499/220

var_ratio;n_ratio
```
The variance of the estimated mean difference in part a is 12.12. The variance of the estimated mean difference in part b is 6.6. The ratio of the two sample sizes (499/220) is 2.25, which is close to the ratio of the two variances, which is 1.8. As the variance of the differences increases, so does the needed sample size to detect that difference, and they increase at about the same rate. 

##d

If you assume that the difference in means estimated in part a and part b are the same, then you are assuming that the performance of the dropout cases would have been the same as the performance of those who took the assessment in January and July (because the part (a) calculation includes scores for people that only took the assessment at one time point, while the part (b) calculation only includes people who took the assessment at both time points). This is a problematic assumption because some of the cited reasons for non-participation in July (absenteeism and turnover) might be correlated with poorer performance on the assessment. People who leave the company after the January assessment may have left for performance reasons, and absenteeism on the job may be related to people having weaker skills in the software being tested because they've had less training or opportunity to use them. 
